\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{polski}
\usepackage[T1]{fontenc}
 
\usepackage[margin=1.5in]{geometry} 

\usepackage{color} 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}                                                                   
\usepackage{graphicx}                                                             
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{pdfpages}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{etoolbox}

\makeatletter
\newenvironment{definition}[1]{%
    \trivlist
    \item[\hskip\labelsep\textbf{Definition. #1.}]
    \ignorespaces
}{%
    \endtrivlist
}
\newenvironment{fact}[1]{%
    \trivlist
    \item[\hskip\labelsep\textbf{Fact. #1.}]
    \ignorespaces
}{%
    \endtrivlist
}
\newenvironment{theorem}[1]{%
    \trivlist
    \item[\hskip\labelsep\textbf{Theorem. #1.}]
    \ignorespaces
}{%
    \endtrivlist
}
\newenvironment{information}[1]{%
    \trivlist
    \item[\hskip\labelsep\textbf{Information. #1.}]
    \ignorespaces
}{%
    \endtrivlist
}
\newenvironment{identities}[1]{%
    \trivlist
    \item[\hskip\labelsep\textbf{Identities. #1.}]
    \ignorespaces
}{%
    \endtrivlist
}
\makeatother

\title{AiSD}  
\author{Rafał Włodarczyk}
\date{INA 4, 2025}

\begin{document}

\maketitle

\tableofcontents

\section{Lecture I - Sortowanie}

Definiujemy problem:

\begin{enumerate}
    \item Input: $A=(a_1,\dots,a_n), |A|=n$
    \item Output: Permutacja tablicy wyjściowej $(a_1',a_2',\dots,a_n')$, takie że: $a_1'\leq a_2' \leq \dots \leq a_n'$.
\end{enumerate}

\subsection{Worst-case analysis}

\begin{align}
    T(n) = \max_{\text{wszystkie wejścia}}\{\text{\#operacji po wszystkich |n|-wejściach}\}
\end{align}

\subsection{Average-case analysis}

Zakładamy pewien rozkład prawdopodobieństwa na danych wejściowych. Z reguły myślimy o rozkładzie jednostajnym. Niech $T$ - zmienna losowa liczby operacji wykonanych przez badany algorytm.

\begin{align}
    \mathbf{E}(T) - \text{wartość oczekiwana T}
\end{align}

\noindent
Później możemy badać wariancję, oraz koncentrację.

\subsection{Analiza losowego sortowania}

Dla poprzedniego algorytmu zobaczmy, że: $n! \sim \sqrt{2\pi n} \left(\frac{n}{e}\right)^n$ $\left[\text{czyli } f(n)\sim g(n) \equiv \lim_{n\rightarrow \infty} \frac{f(n)}{g(n)} = 1 \right]$. To jest tragiczna złożoność.

\subsection{Insertion Sort $(A,n)$}

$(A,n) = ((a_1,a_2,\dots, a_n),n)$

\begin{verbatim}
for j = 2...n
{
    key = A[j]
    i=j-1
    while(i>0 && A[i]>key) {
        A[i+1] = A[i]
        i = i - 1
    }
    A[i+1] = key
}
\end{verbatim}

\noindent
Przykład: $A=(8, 2, 4, 9, 3, 6), n = 6$

\begin{itemize}
    \item $8_i, 2_j, 4, 9, 3, 6 \quad j=2, i=1, key = 2$ while
    \item $2, 8_j, 4, 9, 3, 6$
    \item $2, 8_i, 4_j, 9, 3, 6 \quad j=3, i=2, key = 4$ while
    \item $2, 4, 8, 9, 3, 6$
    \item $2, 4, 8_i, 9_j, 3, 6 \quad j=4, i=3, key = 9$ no while
    \item $2, 4, 8, 9_i, 3_j, 6 \quad j=5, i=4, key = 3$ while
    \item $2, 3, 4, 8, 9, 6$
    \item $2, 3, 4, 8, 9_i ,6_j \quad j=6, i=5, key = 6$ while
    \item $2, 3, 4, 6, 8, 9$
\end{itemize}

\begin{verbatim}
| <= x | > x | x | ... |
| <= x | x | > x | ... |
\end{verbatim}

\noindent
Porównujemy element ze wszystkim co jest przed nim - wszystko przed $j$-tym elementem będzie posortowane. Insertion sort nie swapuje par elementów w tablicy, a przenosi tam gdzie jest jego miejsce.

\subsubsection{Worst-case analysis - Insertion Sort $(A,n)$}

Odwrotnie posortowana tablica powoduje najwięcej przesunięć. Ponieważ ustaliśmy że liczba operacji w while zależy od $j$, wtedy:

\begin{align}
    T(n) &= \sum_{j=2}^n O(j-1) = \sum_{j=1}^{n-1} O(j) = O\left(\sum_{j=1}^{n-1} j\right) =\\
    &= O\left(\frac{1+n-1}{2}\cdot (n-1)\right) = O\left(\frac{(n-1)\cdot(n)}{2}\right) = O\left(\frac{n^2}{2}\right) = O(n^2)
\end{align}
c
\subsubsection{Average-case analysis - Insertion Sort $(A,n)$}

Policzmy dla uproszczenia, że na wejściu mamy $n$-elementowe permutacje, z których każda jest jednakowo prawdopodobna $p=\frac{1}{n!}$. Spróbujmy wyznaczyć $\mathbf{E}$, korzystając z inwersji permutacji. Wartość oczekiwana liczby inwersji w losowej permutacji wynosi:

\begin{align}
    \mathbf{E} \sim \frac{n^2}{4}
\end{align}

\noindent
Pominęliśmy stałe wynikające z innych operacji niż porównywanie. W average-case będziemy około połowę szybiciej niż w worst-case.\\

\noindent
\textit{Pseudokod bez przykładu jest słaby.}

\subsection{Przykład złożoności}

Patrzymy na wiodący czynnik.

\begin{align}
    13n^2 + 91n\log n + 4n + 13^{10} &= O(n^2)\\
    &= 13n^2 + O(n\log n)
\end{align}
\noindent
Chcielibyśmy gdzie to konieczne, zapisać \textit{lower order terms}.\\

\noindent
\textit{Pytanie o dzielenie liczb} - istnieją algorytmy, które ze względu na arytmetyczne właściwości liczb sprawiają, że mniejsze liczby mogą dzielić się dłużej niż większe. Podczas tego kursu nie omawiamy złożoności dla takich algorytmów.  

\section{Lecture II - Merge Sort}

\subsection{Merge sort $(A,1,n)$}

Niech złożoność T(n) - złożność algorytmu.

\newpage

\noindent
Funkcja merge sort
\begin{verbatim}
O(1)          | if |A[1...n]| == 1 return A[1...n]
              | else
T(floor(n/2)) |     B = MERGE_SORT(A,1,floor(n/2))
T(ceil(n/2))  |     C = MERGE_SORT(A,floor(n/2)+1, n)
O(n)          |     return MERGE(B,C)
\end{verbatim}

\noindent
Funkcja merge
\begin{verbatim}
MERGE(X[1...k], Y[1...l])
if k = 0 return Y[1...l]
if l = 0 return X[1...k]
if X[1] <= Y[1]
    return X[1] o MERGE(X[2...k], Y[1...l])
else   
    return Y[1] o MERGE(X[1...k], Y[2...l])
\end{verbatim}

\begin{verbatim}
MERGE(A,B)
2 1 ---> [1] + MERGE(A,B (bez 1))
7 9
13 10
19 11
20 14
  
2 9  ---> [1,2] + MERGE(A (bez 2),B)
7 10
13 11
19 14
20 .

... ---> [1,2,7,9,10,11,13,14]
19 .
20 .

... ---> [1,2,7,9,10,11,13,14,19,20]
\end{verbatim}

\begin{verbatim}
[10], [2], [5], [3], [7], [13], [1], [6]
[2, 10], [3,5], [7,13], [1,6]
[2,3,5,10], [1,6,7,13]
[1,2,3,5,6,7,10,13]
\end{verbatim}

\noindent
Złożoność obliczeniowa merge-a wynosi $O(k+l)$ - w najgorszym przypadku bierzemy najpierw z jednej strony, potem z drugiej i na zmianę.

\begin{align}
    T(n) &= T\left(\left\lfloor \frac{n}{2} \right\rfloor\right) + T\left(\left\lceil \frac{n}{2} \right\rceil\right) + O(n)\\
    T(n) &= 2\cdot T\left(\frac{n}{2}\right) + O(n)
\end{align}

\newpage

\noindent
Rozpiszmy tzw drzewo rekursji:

\begin{verbatim}
        cn                             | cn
       /  \                            |
  cn/2      cn/2                       | cn
  /   \     /   \                      |
cn/4 cn/4 cn/4 cn/4                    | cn
                                       |
      [...]                            | ...
                                       |
O(1) ... O(1) ... O(1) # liści mamy n  | cn
\end{verbatim}

\noindent
Musimy dodać wszystkie koszty, które pojawiły się w drzewie. Dodajmy piętra, a następnie zsumumjmy. 
Żeby znać wysokość drzewa interesuje nas dla jakiego $h$ zajdzie $\frac{n}{2^h} = 1$

\begin{align}
    \frac{n}{2^h} = 1 \implies 2^h = n \implies h = \log_2 n
\end{align}
Zatem złożność:
\begin{align}
    \sum_{i=1}^{\log n} cn = cn\log n \sim O(n\log n)
\end{align}

\section{Lecture III - Narzędzia do analizy algorytmów}

\textit{Dzisiejszy wykład prowadzi GODfryd}

\subsection{Notacja asymptotyczna}

\begin{itemize}
    \item Big-$O$ ($O$-duże) $f: \mathbb{N} \rightarrow \mathbb{R}$
    \item Big-$\Omega$ ($\Omega$-duże) $f: \mathbb{N} \rightarrow \mathbb{R}$
    \item Big-$\Theta$ ($\Theta$-duże) $f: \mathbb{N} \rightarrow \mathbb{R}$
    \item Small-$o$ ($o$-małe) $f: \mathbb{N} \rightarrow \mathbb{R}$
\end{itemize}

\subsection{Notacja Big-$O$}

\begin{definition}{Notacja Big-$O$}
    Funkcja $f(n)\in O(g(n))$, gdy:
    \[
        f(n) = O(g(n)) \equiv 
        \left(\exists c>0\right)
        \left(\exists n_0\in\mathbb{N}\right)
        \left(\forall n\geq n_0\right)
        \left(|f(n)| \leq c \cdot |g(n)|\right)
    \]
    \noindent
    Przykład: $2n^2 = O(n^3)$, dla $n_0 = 2, c = 1$ definicja jest spełniona.\\
    \\
    \textit{Pomijamy tutaj stałe - interesuje nas rząd wielkości}

    \[
        O(g(n)) = \left\{f\in\mathbb{N}^\mathbb{R} : \text{f spełnia definicję}\right\}
    \]

    \noindent
    $O(g(n))$ jest klasą funkcji, ale jako informatycy możemy zapisywać $f=O(g)$, zamiast $f\in O(g)$.
    Notacja nie ma symetrii, to znaczy $f=O(g) \nrightarrow  g=O(f)$ 
\end{definition}

\begin{fact}{Definicja Big-O za pomocą granicy}
    Możemy zapisać alternatywnie:
    \[
        f(n) = O(g(n)) \equiv \limsup_{n\rightarrow \infty} \left|\frac{f(n)}{g(n)}\right| \leq \infty
    \]
    \noindent
    Uwaga. Jeśli $\lim_{n\rightarrow \infty} \left|\frac{f(n)}{g(n)}\right| < \infty$ (istnieje), to:
    \[
        \limsup_{n\rightarrow \infty} \left|\frac{f(n)}{g(n)}\right| =  \lim_{n\rightarrow \infty} \left|\frac{f(n)}{g(n)}\right| 
    \]
\end{fact}

Przykłady:
$
\begin{cases}
    f(n) = n^2\\
    g(n) = (-1)^n n^2
\end{cases}
$


Granica nie istnieje, ale $\limsup = 1$

$
\begin{cases}
    \frac{f(n)}{g(n)} = \begin{cases}
        1, \quad 2 \quad | \quad n\\
        \frac{1}{n}, \quad 2 \quad \not| \quad n
    \end{cases}
\end{cases}
$

Granica nie istnieje.

\begin{fact}{Dokładność zapisu Big-O}
    Pomijamy składniki niższego rzędu jako mniej istotne, ale podkreślamy że istnieją:
    \begin{align}
        f(n) = n^3 + O(n^2) \equiv 
        \left(\exists h(n) = O(n^2)\right)\left(f(n) = n^3 + h(n)\right)
    \end{align}

    \noindent
    Rozważmy następnie stwierdzenie:
    \begin{align}
        n^2 + O(n) = O(n^2) \equiv 
        \left(\forall f(n) = O(n)\right)
        \left(\exists h(n) = O(n^2)\right)
        \left(n^2 + f(n) = h(n)\right)
    \end{align}
    Rozumiemy to następująco - dodając dowolną funkcję z klasy funkcji liniowych do $n^2$ otrzymamy funkcję z klasy funkcji kwadratowych.
\end{fact}

\subsection{Notacja Big-$\Omega$}

\begin{definition}{Notacja Big-$\Omega$}
    Funkcja $f(n)\in \Omega(g(n))$, gdy:
    \begin{align}
        f(n) = \Omega(g(n)) \equiv 
        \left(\exists c>0\right)
        \left(\exists n_0\in\mathbb{N}\right)
        \left(\forall n\geq n_0\right)
        \left(|f(n)| \geq c \cdot |g(n)|\right)
    \end{align}
    \noindent
    biorąc $c' = \frac{1}{c} > 0$ mamy: $(|g(n)| \leq c' \cdot |f(n)|)$, czyli $g(n) = O(f(n))$.\\
    Przykład:
    \begin{align}
        2n^2 = O(n^3)\\
        n^3 = \Omega(2n^2)\\
        n = \Omega(\log n)
    \end{align}

    \noindent
    \textit{Każda funkcja jest Omega od siebie samej.}
\end{definition}

\subsection{Notacja Big-$\Theta$}

\begin{definition}{Notacja Big-$\Theta$}
    Funkcja $f(n)\in \Theta(g(n))$, gdy:
    \begin{align}
        f(n) = \Theta(g(n)) \equiv 
        \left(\exists c_1, c_2 > 0\right)
        \left(\exists n_0\in\mathbb{N}\right)
        \left(\forall n\geq n_0\right)
        \left(c_1 \cdot |g(n)| \leq |f(n)| \leq c_2 \cdot |g(n)|\right)
    \end{align}
    \noindent
    Przykład:
    \begin{align}
        n^2 = \Theta(2n^2)\\
        n^3 = \Theta(n^3)\\
        n^4 + 3n^2 + \log n = \Theta(n^4)
    \end{align}
\end{definition}

\begin{fact}{Dokładność zapisu Theta}
    \begin{align}
        f(n) = \Theta(g(n)) \equiv f(n) = O(g(n)) \land f(n) = \Omega(g(n))\\
        \Theta(f(n)) = O(f(n)) \cap \Omega(f(n))
    \end{align}
\end{fact}
Rozważmy przypadek patologiczny
\begin{align}
    f(n) = n^{1+\sin \frac{\pi \cdot n}{2}}\quad g(n) = n\\
    f\neq O(g), g\neq O(f)
\end{align}

\subsection{Notacja small-$o$}

\begin{definition}{Notacja small-$o$}
    Funkcja $f(n)\in o(g(n))$, gdy:
    \begin{align}
        f(n) = o(g(n)) \equiv 
        \left(\forall c>0\right)
        \left(\exists n_0\in\mathbb{N}\right)
        \left(\forall n\geq n_0\right)
        \left(|f(n)| < c \cdot |g(n)|\right)
    \end{align}
    \noindent
    Równoważnie:
    \begin{align}
        f(n) = o(g(n)) \equiv \lim_{n\rightarrow \infty} \left|\frac{f(n)}{g(n)}\right| = 0
    \end{align}
    \noindent
    Przykład:
    \begin{align}
        n = o(n^2)\\
        n^2 = o(n^3)\\
        n^3 = o(2^n)
    \end{align}
\end{definition}

\subsection{Notacja small-$\omega$}

\begin{definition}{Notacja small-$\omega$}
    Funkcja $f(n)\in \omega(g(n))$, gdy:
    \begin{align}
        f(n) = \omega(g(n)) \equiv 
        \left(\forall c>0\right)
        \left(\exists n_0\in\mathbb{N}\right)
        \left(\forall n\geq n_0\right)
        \left(|f(n)| > c \cdot |g(n)|\right)
    \end{align}
    \noindent
    Równoważnie:
    \begin{align}
        f(n) = \omega(g(n)) \equiv \lim_{n\rightarrow \infty} \left|\frac{f(n)}{g(n)}\right| = \infty
    \end{align}
    \noindent
    Przykład:
    \begin{align}
        3.14n^2 + n = O(n^3) = \omega(n)
    \end{align}
\end{definition}

\subsection{Metody rozwiązywania rekurencji}

\begin{itemize}
    \item Metoda podstawienia (indukcji) - Cormen
    \item Metoda drzewa rekursji
    \item Metoda master theorem
\end{itemize}

\subsection{Rozwiązywanie rekurencji}

\begin{enumerate}
    \item Zgadnij odpowiedź (wiodący składnik)
    \item Sprawdź przez indukcję, czy dobrze zgadliśmy
    \item Wylicz stałe
\end{enumerate}

\begin{information}{Historyjka}
    Dwóch przyjaciół zgubiło się podczas podróży balonem.
    \begin{itemize}
        \item "Gdzie jesteśmy?"
        \item "W balonie."
    \end{itemize}
    Osoba, którą spotkali, była matematykiem.\\ Odpowiedź była precyzyjna, dokładna i całkowicie bezużyteczna.
\end{information}

\subsection{Metoda podstawiania - Metoda dowodu indukcyjnego}

Przykład 1. Rozwiążmy równanie rekurencyjne:
\begin{align}
    T(n) = 4T\left(\frac{n}{2}\right) + n \quad T(1) = \Theta(1)
\end{align}
Załóżmy, że $T(n)=O(n^3)$ - pokazać, że $T(n)\leq c\cdot n^3$. dla dużych $n$. 

\begin{enumerate}
    \item Krok początkowy $T(1) = \Theta(1) \leq c\cdot 1^3 = c$ ok.
    \item Założmy, że $\forall_{k<n}  T(k) \leq c\cdot k^3$ (zał. indukcyjne, nie $\Theta(k^3)$ - chcemy konkretną stałą $c$)
    \item $T(n) = 4T\left(\frac{n}{2}\right) + n \leq 4c\left(\frac{n}{2}\right)^3 + n = \frac{1}{2}cn^3 + n = cn^3 - \frac{1}{2}cn^3 + n \leq cn^3$.
    \item Wystarczy wskazać $c$, takie że $\frac{1}{2}cn^3 - n \geq 0$, np $c\geq 2$
    \item Pokazaliśmy, że $T(n) = O(n^3)$
\end{enumerate}
Załóżmy, że $T(n)=O(n^2)$ - pokazać, że $T(n)\leq c\cdot n^2$. dla dużych $n$.
\begin{enumerate}
    \item Krok początkowy $T(1) = \Theta(1) \leq c\cdot 1^2 = c$ ok.
    \item Założmy, że $\forall_{k<n}  T(k) \leq c\cdot k^2$ (zał. indukcyjne)
    \item $T(n) = 4T\left(\frac{n}{2}\right) + n \leq 4c\left(\frac{n}{2}\right)^2 + n = cn^2 + n = cn^2 - cn^2 + n \leq cn^2$.
    \item Tego się nie da pokazać - nie jest prawdą, że $T(n) = O(n^2)$
\end{enumerate}
Wzmocnijmy zatem założenie indukcyjne:
\begin{enumerate}
    \item $T(n) \leq c_1 n^2 - c_2 n$ (zał. indukcyjne)
    \item $T(n) = 4T\left(\frac{n}{2}\right) + n \leq 4(c_1 \frac{n}{2}^2 - c_2 \frac{n}{2}) + n$
    \item $= c_1 n^2 - 2c_2 n + n = c_1n^2 - (2c_2 -1)n \leq$
    \item $\leq c_1n^2 - c_2n$
    \item Weźmy $c_1=1, c_2=2$, wtedy $T(n) \leq n^2 - 2n = O(n^2)$
\end{enumerate}

\noindent
Przykład 2. Weźmy paskudną rekursję $T(n) = 2T(\sqrt{n}) + \log n$.\\
Załóżmy, że $n$ jest potęgą $2$ oraz oznaczny $n=2^m, m=\log_2 n$.
\begin{align}
    T(2^m) = 2T((2^m)^\frac{1}{2}) + m
\end{align}
Oznaczmy $T(2^m) = S(m)$. Wtedy:
\begin{align}
    S(m) = 2S\left(\frac{m}{2}\right) + m
\end{align}
(dobrze znana rekurencja - $S(n) = O(m\log m))$ - patrz Lecture 2.
Przejdźmy z powrotem na $T,n$:
\begin{align}
    T(2^m) = S(m)
    T(2^m) = O(m \log m)
    T(n) = O(\log n \log \log n)
\end{align}
Formalnie pokazaliśmy to tylko dla potęg $2$ - musielibyśmy jeszcze indukcyjnie to udowodnić.\\

\textit{Kiedy podłogi i sufity mają znaczenie?}

\end{document}
