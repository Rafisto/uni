\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{polski}
\usepackage[T1]{fontenc}
 
\usepackage[margin=1.5in]{geometry} 

\usepackage{color} 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}                                                                   
\usepackage{graphicx}                                                             
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{pdfpages}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{etoolbox}

\makeatletter
\newenvironment{definition}[1]{%
    \trivlist
    \item[\hskip\labelsep\textbf{Definition. #1.}]
    \ignorespaces
}{%
    \endtrivlist
}
\newenvironment{fact}[1]{%
    \trivlist
    \item[\hskip\labelsep\textbf{Fact. #1.}]
    \ignorespaces
}{%
    \endtrivlist
}
\newenvironment{theorem}[1]{%
    \trivlist
    \item[\hskip\labelsep\textbf{Theorem. #1.}]
    \ignorespaces
}{%
    \endtrivlist
}
\newenvironment{information}[1]{%
    \trivlist
    \item[\hskip\labelsep\textbf{Information. #1.}]
    \ignorespaces
}{%
    \endtrivlist
}
\newenvironment{identities}[1]{%
    \trivlist
    \item[\hskip\labelsep\textbf{Identities. #1.}]
    \ignorespaces
}{%
    \endtrivlist
}
\makeatother

\title{AiSD}  
\author{Rafał Włodarczyk}
\date{INA 4, 2025}

\begin{document}

\maketitle

\tableofcontents

\begin{center}
    \textit{I welcome you on the path to insanity.}
\end{center}

\begin{center}
    \textit{Good luck :)}
\end{center}

\newpage

\section{Lecture I - Sortowanie}

Definiujemy problem:

\begin{enumerate}
    \item Input: $A=(a_1,\dots,a_n), |A|=n$
    \item Output: Permutacja tablicy wyjściowej $(a_1',a_2',\dots,a_n')$, takie że: $a_1'\leq a_2' \leq \dots \leq a_n'$.
\end{enumerate}

\subsection{Worst-case analysis}

\begin{align}
    T(n) = \max_{\text{wszystkie wejścia}}\{\text{\#operacji po wszystkich |n|-wejściach}\}
\end{align}

\subsection{Average-case analysis}

Zakładamy pewien rozkład prawdopodobieństwa na danych wejściowych. Z reguły myślimy o rozkładzie jednostajnym. Niech $T$ - zmienna losowa liczby operacji wykonanych przez badany algorytm.

\begin{align}
    \mathbf{E}(T) - \text{wartość oczekiwana T}
\end{align}

\noindent
Później możemy badać wariancję, oraz koncentrację.

\subsection{Analiza losowego sortowania}

Dla poprzedniego algorytmu zobaczmy, że: $n! \sim \sqrt{2\pi n} \left(\frac{n}{e}\right)^n$ $\left[\text{czyli } f(n)\sim g(n) \equiv \lim_{n\rightarrow \infty} \frac{f(n)}{g(n)} = 1 \right]$. To jest tragiczna złożoność.

\subsection{Insertion Sort $(A,n)$}

$(A,n) = ((a_1,a_2,\dots, a_n),n)$

\begin{verbatim}
for j = 2...n
{
    key = A[j]
    i=j-1
    while(i>0 && A[i]>key) {
        A[i+1] = A[i]
        i = i - 1
    }
    A[i+1] = key
}
\end{verbatim}

\noindent
Przykład: $A=(8, 2, 4, 9, 3, 6), n = 6$

\begin{itemize}
    \item $8_i, 2_j, 4, 9, 3, 6 \quad j=2, i=1, key = 2$ while
    \item $2, 8_j, 4, 9, 3, 6$
    \item $2, 8_i, 4_j, 9, 3, 6 \quad j=3, i=2, key = 4$ while
    \item $2, 4, 8, 9, 3, 6$
    \item $2, 4, 8_i, 9_j, 3, 6 \quad j=4, i=3, key = 9$ no while
    \item $2, 4, 8, 9_i, 3_j, 6 \quad j=5, i=4, key = 3$ while
    \item $2, 3, 4, 8, 9, 6$
    \item $2, 3, 4, 8, 9_i ,6_j \quad j=6, i=5, key = 6$ while
    \item $2, 3, 4, 6, 8, 9$
\end{itemize}

\begin{verbatim}
| <= x | > x | x | ... |
| <= x | x | > x | ... |
\end{verbatim}

\noindent
Porównujemy element ze wszystkim co jest przed nim - wszystko przed $j$-tym elementem będzie posortowane. Insertion sort nie swapuje par elementów w tablicy, a przenosi tam gdzie jest jego miejsce.

\subsubsection{Worst-case analysis - Insertion Sort $(A,n)$}

Odwrotnie posortowana tablica powoduje najwięcej przesunięć. Ponieważ ustaliśmy że liczba operacji w while zależy od $j$, wtedy:

\begin{align}
    T(n) &= \sum_{j=2}^n O(j-1) = \sum_{j=1}^{n-1} O(j) = O\left(\sum_{j=1}^{n-1} j\right) =\\
    &= O\left(\frac{1+n-1}{2}\cdot (n-1)\right) = O\left(\frac{(n-1)\cdot(n)}{2}\right) = O\left(\frac{n^2}{2}\right) = O(n^2)
\end{align}
c
\subsubsection{Average-case analysis - Insertion Sort $(A,n)$}

Policzmy dla uproszczenia, że na wejściu mamy $n$-elementowe permutacje, z których każda jest jednakowo prawdopodobna $p=\frac{1}{n!}$. Spróbujmy wyznaczyć $\mathbf{E}$, korzystając z inwersji permutacji. Wartość oczekiwana liczby inwersji w losowej permutacji wynosi:

\begin{align}
    \mathbf{E} \sim \frac{n^2}{4}
\end{align}

\noindent
Pominęliśmy stałe wynikające z innych operacji niż porównywanie. W average-case będziemy około połowę szybiciej niż w worst-case.\\

\noindent
\textit{Pseudokod bez przykładu jest słaby.}

\subsection{Przykład złożoności}

Patrzymy na wiodący czynnik.

\begin{align}
    13n^2 + 91n\log n + 4n + 13^{10} &= O(n^2)\\
    &= 13n^2 + O(n\log n)
\end{align}
\noindent
Chcielibyśmy gdzie to konieczne, zapisać \textit{lower order terms}.\\

\noindent
\textit{Pytanie o dzielenie liczb} - istnieją algorytmy, które ze względu na arytmetyczne właściwości liczb sprawiają, że mniejsze liczby mogą dzielić się dłużej niż większe. Podczas tego kursu nie omawiamy złożoności dla takich algorytmów.  

\section{Lecture II - Merge Sort}

\subsection{Merge sort $(A,1,n)$}

Niech złożoność T(n) - złożność algorytmu.

\noindent
Funkcja merge sort
\begin{verbatim}
O(1)          | if |A[1...n]| == 1 return A[1...n]
              | else
T(floor(n/2)) |     B = MERGE_SORT(A,1,floor(n/2))
T(ceil(n/2))  |     C = MERGE_SORT(A,floor(n/2)+1, n)
O(n)          |     return MERGE(B,C)
\end{verbatim}

\noindent
Funkcja merge
\begin{verbatim}
MERGE(X[1...k], Y[1...l])
if k = 0 return Y[1...l]
if l = 0 return X[1...k]
if X[1] <= Y[1]
    return X[1] o MERGE(X[2...k], Y[1...l])
else   
    return Y[1] o MERGE(X[1...k], Y[2...l])
\end{verbatim}

\begin{verbatim}
MERGE(A,B)
2 1 ---> [1] + MERGE(A,B (bez 1))
7 9
13 10
19 11
20 14
  
2 9  ---> [1,2] + MERGE(A (bez 2),B)
7 10
13 11
19 14
20 .

... ---> [1,2,7,9,10,11,13,14]
19 .
20 .

... ---> [1,2,7,9,10,11,13,14,19,20]
\end{verbatim}

\begin{verbatim}
[10], [2], [5], [3], [7], [13], [1], [6]
[2, 10], [3,5], [7,13], [1,6]
[2,3,5,10], [1,6,7,13]
[1,2,3,5,6,7,10,13]
\end{verbatim}

\noindent
Złożoność obliczeniowa merge-a wynosi $O(k+l)$ - w najgorszym przypadku bierzemy najpierw z jednej strony, potem z drugiej i na zmianę.

\begin{align}
    T(n) &= T\left(\left\lfloor \frac{n}{2} \right\rfloor\right) + T\left(\left\lceil \frac{n}{2} \right\rceil\right) + O(n)\\
    T(n) &= 2\cdot T\left(\frac{n}{2}\right) + O(n)
\end{align}

\newpage

\noindent
Rozpiszmy tzw drzewo rekursji:

\begin{verbatim}
        cn                             | cn
       /  \                            |
  cn/2      cn/2                       | cn
  /   \     /   \                      |
cn/4 cn/4 cn/4 cn/4                    | cn
                                       |
      [...]                            | ...
                                       |
O(1) ... O(1) ... O(1) # liści mamy n  | cn
\end{verbatim}

\noindent
Musimy dodać wszystkie koszty, które pojawiły się w drzewie. Dodajmy piętra, a następnie zsumumjmy. 
Żeby znać wysokość drzewa interesuje nas dla jakiego $h$ zajdzie $\frac{n}{2^h} = 1$

\begin{align}
    \frac{n}{2^h} = 1 \implies 2^h = n \implies h = \log_2 n
\end{align}
Zatem złożność:
\begin{align}
    \sum_{i=1}^{\log n} cn = cn\log n \sim O(n\log n)
\end{align}

\section{Lecture III - Narzędzia do analizy algorytmów}

\textit{Dzisiejszy wykład prowadzi GODfryd}

\subsection{Notacja asymptotyczna}

\begin{itemize}
    \item Big-$O$ ($O$-duże) $f: \mathbb{N} \rightarrow \mathbb{R}$
    \item Big-$\Omega$ ($\Omega$-duże) $f: \mathbb{N} \rightarrow \mathbb{R}$
    \item Big-$\Theta$ ($\Theta$-duże) $f: \mathbb{N} \rightarrow \mathbb{R}$
    \item Small-$o$ ($o$-małe) $f: \mathbb{N} \rightarrow \mathbb{R}$
\end{itemize}

\subsection{Notacja Big-$O$}

\begin{definition}{Notacja Big-$O$}
    Funkcja $f(n)\in O(g(n))$, gdy:
    \[
        f(n) = O(g(n)) \equiv 
        \left(\exists c>0\right)
        \left(\exists n_0\in\mathbb{N}\right)
        \left(\forall n\geq n_0\right)
        \left(|f(n)| \leq c \cdot |g(n)|\right)
    \]
    \noindent
    Przykład: $2n^2 = O(n^3)$, dla $n_0 = 2, c = 1$ definicja jest spełniona.\\
    \\
    \textit{Pomijamy tutaj stałe - interesuje nas rząd wielkości}

    \[
        O(g(n)) = \left\{f\in\mathbb{N}^\mathbb{R} : \text{f spełnia definicję}\right\}
    \]

    \noindent
    $O(g(n))$ jest klasą funkcji, ale jako informatycy możemy zapisywać $f=O(g)$, zamiast $f\in O(g)$.
    Notacja nie ma symetrii, to znaczy $f=O(g) \nrightarrow  g=O(f)$ 
\end{definition}

\begin{fact}{Definicja Big-O za pomocą granicy}
    Możemy zapisać alternatywnie:
    \[
        f(n) = O(g(n)) \equiv \limsup_{n\rightarrow \infty} \left|\frac{f(n)}{g(n)}\right| \leq \infty
    \]
    \noindent
    Uwaga. Jeśli $\lim_{n\rightarrow \infty} \left|\frac{f(n)}{g(n)}\right| < \infty$ (istnieje), to:
    \[
        \limsup_{n\rightarrow \infty} \left|\frac{f(n)}{g(n)}\right| =  \lim_{n\rightarrow \infty} \left|\frac{f(n)}{g(n)}\right| 
    \]
\end{fact}

Przykłady:
$
\begin{cases}
    f(n) = n^2\\
    g(n) = (-1)^n n^2
\end{cases}
$


Granica nie istnieje, ale $\limsup = 1$

$
\begin{cases}
    \frac{f(n)}{g(n)} = \begin{cases}
        1, \quad 2 \quad | \quad n\\
        \frac{1}{n}, \quad 2 \quad \not| \quad n
    \end{cases}
\end{cases}
$

Granica nie istnieje.

\begin{fact}{Dokładność zapisu Big-O}
    Pomijamy składniki niższego rzędu jako mniej istotne, ale podkreślamy że istnieją:
    \begin{align}
        f(n) = n^3 + O(n^2) \equiv 
        \left(\exists h(n) = O(n^2)\right)\left(f(n) = n^3 + h(n)\right)
    \end{align}

    \noindent
    Rozważmy następnie stwierdzenie:
    \begin{align}
        n^2 + O(n) = O(n^2) \equiv 
        \left(\forall f(n) = O(n)\right)
        \left(\exists h(n) = O(n^2)\right)
        \left(n^2 + f(n) = h(n)\right)
    \end{align}
    Rozumiemy to następująco - dodając dowolną funkcję z klasy funkcji liniowych do $n^2$ otrzymamy funkcję z klasy funkcji kwadratowych.
\end{fact}

\subsection{Notacja Big-$\Omega$}

\begin{definition}{Notacja Big-$\Omega$}
    Funkcja $f(n)\in \Omega(g(n))$, gdy:
    \begin{align}
        f(n) = \Omega(g(n)) \equiv 
        \left(\exists c>0\right)
        \left(\exists n_0\in\mathbb{N}\right)
        \left(\forall n\geq n_0\right)
        \left(|f(n)| \geq c \cdot |g(n)|\right)
    \end{align}
    \noindent
    biorąc $c' = \frac{1}{c} > 0$ mamy: $(|g(n)| \leq c' \cdot |f(n)|)$, czyli $g(n) = O(f(n))$.\\
    Przykład:
    \begin{align}
        2n^2 = O(n^3)\\
        n^3 = \Omega(2n^2)\\
        n = \Omega(\log n)
    \end{align}

    \noindent
    \textit{Każda funkcja jest Omega od siebie samej.}
\end{definition}

\subsection{Notacja Big-$\Theta$}

\begin{definition}{Notacja Big-$\Theta$}
    Funkcja $f(n)\in \Theta(g(n))$, gdy:
    \begin{align}
        f(n) = \Theta(g(n)) \equiv 
        \left(\exists c_1, c_2 > 0\right)
        \left(\exists n_0\in\mathbb{N}\right)
        \left(\forall n\geq n_0\right)
        \left(c_1 \cdot |g(n)| \leq |f(n)| \leq c_2 \cdot |g(n)|\right)
    \end{align}
    \noindent
    Przykład:
    \begin{align}
        n^2 = \Theta(2n^2)\\
        n^3 = \Theta(n^3)\\
        n^4 + 3n^2 + \log n = \Theta(n^4)
    \end{align}
\end{definition}

\begin{fact}{Dokładność zapisu Theta}
    \begin{align}
        f(n) = \Theta(g(n)) \equiv f(n) = O(g(n)) \land f(n) = \Omega(g(n))\\
        \Theta(f(n)) = O(f(n)) \cap \Omega(f(n))
    \end{align}
\end{fact}
Rozważmy przypadek patologiczny
\begin{align}
    f(n) = n^{1+\sin \frac{\pi \cdot n}{2}}\quad g(n) = n\\
    f\neq O(g), g\neq O(f)
\end{align}

\subsection{Notacja small-$o$}

\begin{definition}{Notacja small-$o$}
    Funkcja $f(n)\in o(g(n))$, gdy:
    \begin{align}
        f(n) = o(g(n)) \equiv 
        \left(\forall c>0\right)
        \left(\exists n_0\in\mathbb{N}\right)
        \left(\forall n\geq n_0\right)
        \left(|f(n)| < c \cdot |g(n)|\right)
    \end{align}
    \noindent
    Równoważnie:
    \begin{align}
        f(n) = o(g(n)) \equiv \lim_{n\rightarrow \infty} \left|\frac{f(n)}{g(n)}\right| = 0
    \end{align}
    \noindent
    Przykład:
    \begin{align}
        n = o(n^2)\\
        n^2 = o(n^3)\\
        n^3 = o(2^n)
    \end{align}
\end{definition}

\subsection{Notacja small-$\omega$}

\begin{definition}{Notacja small-$\omega$}
    Funkcja $f(n)\in \omega(g(n))$, gdy:
    \begin{align}
        f(n) = \omega(g(n)) \equiv 
        \left(\forall c>0\right)
        \left(\exists n_0\in\mathbb{N}\right)
        \left(\forall n\geq n_0\right)
        \left(|f(n)| > c \cdot |g(n)|\right)
    \end{align}
    \noindent
    Równoważnie:
    \begin{align}
        f(n) = \omega(g(n)) \equiv \lim_{n\rightarrow \infty} \left|\frac{f(n)}{g(n)}\right| = \infty
    \end{align}
    \noindent
    Przykład:
    \begin{align}
        3.14n^2 + n = O(n^3) = \omega(n)
    \end{align}
\end{definition}

\subsection{Metody rozwiązywania rekurencji}

\begin{itemize}
    \item Metoda podstawienia (indukcji) - Cormen
    \item Metoda drzewa rekursji
    \item Metoda master theorem
\end{itemize}

\subsection{Rozwiązywanie rekurencji}

\begin{enumerate}
    \item Zgadnij odpowiedź (wiodący składnik)
    \item Sprawdź przez indukcję, czy dobrze zgadliśmy
    \item Wylicz stałe
\end{enumerate}

\begin{information}{Historyjka}
    Dwóch przyjaciół zgubiło się podczas podróży balonem.
    \begin{itemize}
        \item "Gdzie jesteśmy?"
        \item "W balonie."
    \end{itemize}
    Osoba, którą spotkali, była matematykiem.\\ Odpowiedź była precyzyjna, dokładna i całkowicie bezużyteczna.
\end{information}

\subsection{Metoda podstawiania - Metoda dowodu indukcyjnego}

Przykład 1. Rozwiążmy równanie rekurencyjne:
\begin{align}
    T(n) = 4T\left(\frac{n}{2}\right) + n \quad T(1) = \Theta(1)
\end{align}
Załóżmy, że $T(n)=O(n^3)$ - pokazać, że $T(n)\leq c\cdot n^3$. dla dużych $n$. 

\begin{enumerate}
    \item Krok początkowy $T(1) = \Theta(1) \leq c\cdot 1^3 = c$ ok.
    \item Założmy, że $\forall_{k<n}  T(k) \leq c\cdot k^3$ (zał. indukcyjne, nie $\Theta(k^3)$ - chcemy konkretną stałą $c$)
    \item $T(n) = 4T\left(\frac{n}{2}\right) + n \leq 4c\left(\frac{n}{2}\right)^3 + n = \frac{1}{2}cn^3 + n = cn^3 - \frac{1}{2}cn^3 + n \leq cn^3$.
    \item Wystarczy wskazać $c$, takie że $\frac{1}{2}cn^3 - n \geq 0$, np $c\geq 2$
    \item Pokazaliśmy, że $T(n) = O(n^3)$
\end{enumerate}
Załóżmy, że $T(n)=O(n^2)$ - pokazać, że $T(n)\leq c\cdot n^2$. dla dużych $n$.
\begin{enumerate}
    \item Krok początkowy $T(1) = \Theta(1) \leq c\cdot 1^2 = c$ ok.
    \item Założmy, że $\forall_{k<n}  T(k) \leq c\cdot k^2$ (zał. indukcyjne)
    \item $T(n) = 4T\left(\frac{n}{2}\right) + n \leq 4c\left(\frac{n}{2}\right)^2 + n = cn^2 + n = cn^2 - cn^2 + n \leq cn^2$.
    \item Tego się nie da pokazać - nie jest prawdą, że $T(n) = O(n^2)$
\end{enumerate}
Wzmocnijmy zatem założenie indukcyjne:
\begin{enumerate}
    \item $T(n) \leq c_1 n^2 - c_2 n$ (zał. indukcyjne)
    \item $T(n) = 4T\left(\frac{n}{2}\right) + n \leq 4(c_1 \frac{n}{2}^2 - c_2 \frac{n}{2}) + n$
    \item $= c_1 n^2 - 2c_2 n + n = c_1n^2 - (2c_2 -1)n \leq$
    \item $\leq c_1n^2 - c_2n$
    \item Weźmy $c_1=1, c_2=2$, wtedy $T(n) \leq n^2 - 2n = O(n^2)$
\end{enumerate}

\noindent
Przykład 2. Weźmy paskudną rekursję $T(n) = 2T(\sqrt{n}) + \log n$.\\
Załóżmy, że $n$ jest potęgą $2$ oraz oznaczny $n=2^m, m=\log_2 n$.
\begin{align}
    T(2^m) = 2T((2^m)^\frac{1}{2}) + m
\end{align}
Oznaczmy $T(2^m) = S(m)$. Wtedy:
\begin{align}
    S(m) = 2S\left(\frac{m}{2}\right) + m
\end{align}
(dobrze znana rekurencja - $S(n) = O(m\log m))$ - patrz Lecture 2.
Przejdźmy z powrotem na $T,n$:
\begin{align}
    T(2^m) = S(m)
    T(2^m) = O(m \log m)
    T(n) = O(\log n \log \log n)
\end{align}
Formalnie pokazaliśmy to tylko dla potęg $2$ - musielibyśmy jeszcze indukcyjnie to udowodnić.\\

\textit{Kiedy podłogi i sufity mają znaczenie?}

\section{Lecture IV - Metoda drzewa rekursji}

\subsection{Metoda drzewa rekursji}

W danym węźle wstawiamy koszt operacji. Sumujemy koszty węzłów na danym poziomie.\\

\begin{align}
    T(n) = T\left(\frac{n}{2}\right) + T\left(\frac{n}{4}\right) + n^2, \quad T(1) = \Theta(1)
\end{align}

\noindent
Chcemy sumować koszty na danym poziomie, a potem napisać pełną sumę.

\begin{verbatim}
              n^2                  | n^2
         /         \ 
   (n/2)^2         (n/4)^2         | 5/16 n^2
  /      \          /     \
(n/4)^2 (n/8)^2 (n/8)^2 (n/16)^2   | 25/256 n^2 = (5/16)^k n^2

...
\end{verbatim}

\begin{align}
    T^{*}(n) &= \sum_{k=0}^{\infty} \left(\frac{5}{16}\right)^k n^2 = \\
    &= n^2 \sum_{k=0}^{\infty} \left(\frac{5}{16}\right)^k = \\
    &= n^2 \cdot \left(\frac{1}{1-\frac{5}{16}}\right) = \\
    &= \frac{16}{11} n^2
\end{align}

\noindent
Nie mogłoby być mniej niż $n^2$, bo już w pierwszym rzędzie jest $n^2$.\\
Nie jest to dokładne, ale dostaliśmy górne ograniczenie.\\

\begin{align}
    T(n) = O(n^2)
\end{align}

\begin{verbatim}
      / \
     / / \
    / /   \ (h)
   / /  (od pewnego momentu nie ma części drzewa)
  / /
 / /
/   
(H)
\end{verbatim}
Wysokości różnią się o stałą:

\begin{align}
    \frac{n}{2^H} = 1 \implies H = \log_2 n\\
    \frac{n}{4^h} = 1 \implies h = \log_4 n
\end{align}

\vspace{1cm}

\noindent
\textit{Za chwilę będę dodawał rzeczy, które nie istnieją}

\vspace{1cm}

\noindent
Pamiętajmy, że:
\[
    a^{\log_b n} = n^{\log_b a}
\]

\begin{align}
    \hat{T}(n) &= \sum_{k=0}^{H=\log_2(n)} \left(\frac{5}{16}\right)^k n^2 =\\
    &= n^2 \sum_{k=0}^{H} \left(\frac{5}{16}\right)^k =\\
    &= n^2 \cdot \frac{1}{11} \left(16 - 5\left(\frac{5}{16}\right)^{\log_2 n}\right) =\\
    &= \frac{16}{11} n^2 - \frac{5}{11} n^{2-1.67}
\end{align}
Rozważmy ograniczenie dolne:
\begin{align}
    \check{T}(n) = \sum_{k=0}^{h=\log_4(n)} \left(\frac{5}{16}\right)^k n^2
    &= n^2 \frac{1}{11}\left(16 - C\cdot \left(\frac{5}{16}\right)^{\log_4 n}\right)
\end{align}
Zatem wiemy, że:
\begin{align}
    T(n) = O(\hat{T}(n)) = O(T^{*}(n))\\
    T(n) = \Omega(\check{T}(n))\\
    T(n) = \Theta(n^2) = \frac{16}{11} n^2 + o(n^2)
\end{align}

\subsection{Metoda iteracyjna}

\begin{align}
    T(n) &= 3T(\left(\frac{n}{4}\right)) + n =\\
    T(n) &= 3\left(3T\left(\left(\frac{n}{16}\right)\right) + \left(\frac{n}{4}\right)\right) + n = 9T\left(\frac{n}{16}\right) + \frac{3}{4} n + n =\\
    T(n) &= n + \frac{3}{4} n + 9\left(3T\left(\frac{n}{64}\right) + \frac{n}{16}\right) =\\
    T(n) &= n + \frac{3}{4} n + \frac{9}{16} n + 27T\left(\frac{n}{64}\right) =\\
    T(n) &= n + \frac{3}{4} n + \left(\frac{3}{4}\right)^2 n + \left(\frac{3}{4}\right)^3 n + \dots + 3^j T\left(\frac{n}{4^j}\right) =\\
\end{align}
Wyznaczmy koniec iteracji:
\begin{align}
    \frac{n}{4^j} = 1 \implies j = \log_4 n
\end{align}
To jest nic innego jak:
\begin{align}
    \sum_{j=0}^{\log_4 n} \left(\frac{3}{4}\right)^j = O(n)
\end{align}

\subsection{Master Theorem}

\begin{theorem}{Master Theorem}
    Jeśli $T(n) = a\cdot T(\lceil \frac{n}{b} \rceil) + \Theta(n^d)$ dla pewnych stałych $a>0, b>1, d>0$, oraz $T(1)=\Theta(1)$ to:
    \[
    T(n) = \begin{cases}
        \Theta\left(n^d\right) \quad \text{jeśli} \quad d > \log_b a\\
        \Theta\left(n^d \log n\right) \quad \text{jeśli} \quad d = \log_b a\\
        \Theta\left(n^{log_b a}\right) \quad \text{jeśli} \quad d < \log_b a
    \end{cases}
    \]
\end{theorem}

\begin{align}
\hat{T} (n) = a\cdot \hat{T} \left(\frac{n}{b} + 1 \right) + \Theta(n^d)\\
\check{T} (n) = a\cdot \check{T} \left(\frac{n}{b} \right)
\end{align}
Dowód
\begin{verbatim}
wielkość               .                   liczba podproblemów    
n                      c n^d               1                       
n/b                    c (n/b)^d           a
n/b^2                  c (n/(b^2))^d       a^2

...

koszt na poziomie 'k' = c (n/b^k)^d
liczba podproblemów na poziomie 'k' = a^k

suma kosztów 'k'-tym wierszu = c (a/b^d)^k * n^d
\end{verbatim}

Wysokość drzewa rekursji
\begin{align}
    \frac{n}{b^h} = 1 \implies h = \log_b n
\end{align}

Zatem:

\begin{align}
    T(n) = \Theta\left(\sum_{k=0}^{\log_b n} \cdot \left(\frac{a}{b^d}\right)^k n^d \right)
\end{align}

\textit{Mogę wziąć thetę zamiast o, bo dość dokładnie robię - ale trochę nie}

\[
    \sum_{k=0}^{h} q^k = \frac{1-q^{h+1}}{1-q} \quad \sum_{h=0}^{h} 1^k = (h+1)
\]

\begin{align}
    T(n) = \Theta\left(n^d \sum_{k=0}^{\log_b n} \cdot \left(\frac{a}{b^d}\right)^k \right)
\end{align}

\noindent
(1) Jeśli $\frac{a}{b^d} < 1$, to:
\begin{align}
    a < b^d \\
    \log_b (a) < d \quad \text{zatem} \\
    T(n) = \Theta(n^d)
\end{align}
(większość pracy dzieje się z korzenia - okolic korzenia)\\

\noindent
(2) Jeśli $\frac{a}{b^d} = 1$, to:
\begin{align}
    a = b^d \\
    \log_b (a) = d \\
    T(n) = \Theta(n^d \log n)
\end{align}
(suma kosztów w $k$-tym wierszu - każdy wiersz kontrybuuje równie mocno)\\

\noindent
(3) Jeśli $\frac{a}{b^d} > 1$, to:
\begin{align}
    a > b^d \\
    \log_b (a) > d \\
    T(n) = \Theta(n^{\log_b a})
\end{align}
(z każdym kolejnym poziomem koszt rośnie - większość złożoności kryje się na dole drzewa rekursji)

\vspace{1cm}

\textit{Z tego co dzieje się na początku... albo na końcu, bo to może być scalanie}\\
\textit{Stworzyliście za dużo podproblemów.}
    
\vspace{1cm}

\noindent
Co jeśli rekurencja nie ma $n^d$, a ma $n\log(n)$? - możemy przybliżać\\

\noindent
Przykład

\begin{align}
    T(n) = 4 T\left(\frac{n}{2}\right) + 11 n \quad a=4, b=2, d=1\\
    \log_b a = \log_2 4 = 2 > 1 = d \quad \text{to jest przypadek (3)}\\
    T(n) = \Theta\left(n^{\log_a b}\right) = \Theta\left(n^{\log_2 4}\right) = \Theta\left(n^2\right)
\end{align}

\noindent
Przykład

\begin{align}
    T(n) = 4 T\left(\frac{n}{3}\right) + 3n^2 \quad a=4, b=3, d=2\\
    \log_b a = \log_3 4 > 2 = d \quad \text{to jest przypadek (1)}\\
    T(n) = \Theta\left(n^d\right) = \Theta\left(n^2\right)
\end{align}

\noindent
Przykład

\begin{align}
    T(n) = 27T\left(\frac{n}{3}\right) + 0.(3) n^3 \quad a=27, b=3, d=3\\
    \log_b a = \log_3 27 = 3 = d \quad \text{to jest przypadek (2)}\\
    T(n) = \Theta\left(n^d \log n\right) = \Theta\left(n^{3} \log n\right)
\end{align}

\subsection{Divide and Conquer}

\begin{enumerate}
    \item Podział problemu na mniejsze podproblemy.
    \item Rozwiąż rekurencyjnie mniejsze (rozłączne) podproblemy.
    \item Połącz rozwiązania problemów w celu rozwiązania problemu wejściowego.
\end{enumerate}

\subsection{Wyszukiwanie elementów w portowanej tablicy}

\begin{itemize}
    \item Input - posortowana tablica $A[1..n]$, element $x$
    \item Output - indeks $i$ taki, że $A[i] = x$ lub błąd, gdy $x$ nie występuje w $A$
\end{itemize}

\subsection{Binary search}

\begin{enumerate}
    \item if $n=1, A[n] = x$ return $n$, else $A$ does not contain $x$
    \item porównujemy $x$ z $A[\frac{n}{2}]$
    \item jeśli $x = A[\frac{n}{2}]$ return $\frac{n}{2}$
    \item jeśli $x < A[\frac{n}{2}]$, \text{BinarySearch}($A[1..\frac{n}{2}-1], x$)
    \item jeśli $x > A[\frac{n}{2}]$, \text{BinarySearch}($A[\frac{n}{2}+1..n], x$)
\end{enumerate}

\vspace{1cm}

\textit{Wy nie patrzcie na pseudokody na tablicy, tylko w książce}

\vspace{1cm}

\begin{align}
    T(n) = 1 \cdot T\left(\frac{n}{2}\right) + \Theta(1) \\
    T(n) = \Theta(\log n)
\end{align}

\section{Lecture V - Divide and Conquer}

\subsection{Potęgowanie liczby}

\begin{itemize}
    \item Input - liczba $x$, liczba całkowita $n$
    \item Output - $x^n$
\end{itemize}
Bazowo zachodzi $n-1$ mnożeń $x$ przez siebie. (czyli $\Theta(n)$ operacji)
\begin{align}
    x \cdot x \cdot \dots \cdot x = x^{n}
\end{align}
Zróbmy to sprytniej:
\begin{align}
    x^n = \begin{cases}
        x^{\frac{n}{2}} \cdot x^{\frac{n}{2}} \quad \text{dla parzystego} \quad n\\
        x^{\frac{n-1}{2}} \cdot x^{\frac{n-1}{2}} \cdot x \quad \text{dla nieparzystego} \quad n
    \end{cases}
\end{align}
Z liniowej liczby mnożeń zeszliśmy do logarytmicznej liczby mnożeń.\\
\begin{align}
    T(n) &= 1\cdot T\left(\frac{n}{2}\right) + \Theta(1)\\
    T(n) &= \Theta(\log n)
\end{align}

\subsection{Wyliczenie $n$-tej liczby Fibonacciego}

\begin{align}
    F_n = \begin{cases}
        0 \quad n = 0\\
        1 \quad n = 1\\
        F(n-1) + F(n-2) ,\quad n > 1
    \end{cases}
\end{align}
Normalne wywołanie funkcji to $\Theta(\varphi^n)$\\\\
Wykorzystajmy podejście bottom-up, liczymy i zapamiętujmy każdorazowo $F_2, F_3,\dots, F_n$\\
Osiągnęliśmy złożoność liniową $\Theta(n)$\\\\
Istnieje jednak zwarty wzór na $F(n)=\frac{1}{\sqrt{5}}\left(\frac{\varphi^n + \varphi^n}{2}\right)$
a to możemy policzyć logarytmicznie.\\

\textit{Tu pojawiają się liczby - jak one się nazywały - (z sali) niewymierne.}\\

\noindent
Istnieje macierz, która mnożona pozwala na policzenie $n$-tej liczby Fibonacciego.

\begin{align}
    \begin{pmatrix}
        1 & 1 \\
        1 & 0
    \end{pmatrix}^n =
    \begin{pmatrix}
        F_{n+1} & F_{n} \\
        F_{n} & F_{n-1}
    \end{pmatrix}
\end{align}
Algorytm używający tego wzoru - połączony z szybkim potęgowaniem, ma złożoność $\Theta(\log n)$.

\subsection{Mnożenie Liczb}

\begin{itemize}
    \item Input: $x,y$ (liczby $n$-bitowe)
    \item Output: $x\cdot y$
\end{itemize}
Standardowe mnożenie w słupku to $\Theta(n^2)$ mnożeń i $\Theta(n)$ dodawań.

\noindent
Załóżmy, że $n$ jest parzyste:

\begin{align}
    x &= x_L \cdot 2^{\frac{n}{2}} + x_R\\
    y &= y_L \cdot 2^{\frac{n}{2}} + y_R\\
    x \cdot y &= \left(x_L \cdot 2^{\frac{n}{2}} + x_R\right) \cdot \left(y_L \cdot 2^{\frac{n}{2}} + y_R\right) =\\
    &= x_L \cdot y_L \cdot 2^n + \left(x_L y_R + x_R y_L\right) \cdot 2^{\frac{n}{2}} + x_R y_R
\end{align}

\begin{align}
    T(n) = 4T\left(\frac{n}{2}\right) + \Theta(n)\\
    a = 4, b = 2, d = 1\\
    \log_b a = \log_2 4 = 2 > 1 = d\\
    T(n) = \Theta(n^2)
\end{align}
\textit{Asymptotycznie nie zyskaliśmy nic.}

\noindent
Ten przypadek pokazuje, że czasami nie wystarczy bezmyślnie podzielić a potem scalić.\\

\noindent
A co o tym myślał Gauss - tu jest dużo mnożeń - cztery.
\begin{align}
    (a+ib)(c+id) = ac - bd + i(bc + ad)\\
    bc + ad = (a + b)(c + d) - ac - bd
\end{align}
Zobaczmy, że $ac, bd$ są już policzone wyżej - zamiast 4 mnożeń, mamy 3 mnożenia.
\begin{align}
    x\cdot y = x_L y_L 2^n + \left((x_L + x_R)(y_L + y_R) - x_Ly_L - x_Ry_R\right) + x_Ry_R
\end{align}
Wykonujemy i zapamiętujemy mnożenia $x_Ly_L, x_Ry_R, (x_L + x_R)(y_L + y_R)$ - zamiast 4 mnożeń, mamy 3 mnożenia.\\

\noindent
$\Theta(n)$ - wynika z przeunięć bitowych oraz dodawań.

\begin{align}
    T(n) = 3T\left(\frac{n}{2}\right) + \Theta(n)\\
    a = 3, b = 2, d = 1\\
    \log_b a = \log_2 3 > 1 = d\\
    T(n) = \Theta(n^{\log_2 3}) = \Theta(n^{1.59})
\end{align}

\noindent
Najszybszy znany algorytm - na podstawie szybkiej transformaty fouriera $\sim O(n\cdot \log n\cdot \log \log n)$

\begin{verbatim}
mutiply(x, y)
    n = max {|x|, |y|}
    if n == 1 return x * y
    x_L, x_R = leftmost(ceil(n/2),x), rightmost(floor(n/2),x)
    y_L, y_R = leftmost(ceil(n/2),y), rightmost(floor(n/2),y)

    p1 = multiply(x_L, y_L)
    p2 = multiply(x_R, y_R)
    p3 = multiply(x_L + x_R, y_L + y_R)

    return p1 << n + (p3 - p1 - p2) << ceil(n/2) + p2
\end{verbatim}

\noindent
Podobnie możemy mnożyć macierze.

\subsection{Mnożenie macierzy}

\begin{itemize}
    \item Input: $A, B$ - $n$-wymiarowe macierze
    \item Output: $A \cdot B$
\end{itemize}

Naiwne mnożenie macierzy wykonuje $\Theta(n^3)$ mnożeń.\\

Podzielmy macierz na 4 równe częsci:

\begin{align}
    \begin{pmatrix}
        A & B\\
        C & D
    \end{pmatrix}
    \times
    \begin{pmatrix}
        E & F\\
        G & H
    \end{pmatrix}
    = 
    \begin{pmatrix}
        AE + BG & AF + BH\\
        CE + DG & CF + DH
    \end{pmatrix}
\end{align}

\begin{align}
    T(n) = 8T\left(\frac{n}{2}\right) + O(n^2)\\
    T(n) = O(n^3)
\end{align}
Znowu nic nie zyskaliśmy. Jesteśmy w stanie wyeliminować jedno mnożenie
- osiągając ostatecznie $\Theta(n^{\log_2 7}) \sim \Theta(n^{2.81})$.\\

Algorytmy state of the art - $\Theta(n^2 \text{polylog}(n))$.

\subsection{Quick Sort}

Algorytm na podział - scalanie już posortowanych. Pozwala na sortowanie w miejscu.

\begin{verbatim}
A[1..n]
|       |------|      |
1       p      q      n

A[1..n]
|       |   <=  |     | <    |      | 
1       p       |pivot|      q      n
\end{verbatim}

\begin{enumerate}
    \item Podziel $A[p..q]$ na dwie tablice:
    $A[p..k-1]$, \textit{pivot}, $A[k+1..q]$
    takie, że:
    \[
    \forall_{i\in[p..k-1]} A[i] \leq \textit{pivot}, \forall_{j\in[k+1..q]} A[j] > \textit{pivot}
    \]
    \item Quicksort($A,p,k-1$)\\
    Quicksort($A,l-1,q$)
\end{enumerate}
Przykład - weźmy nieposortowaną tablicę:
\begin{verbatim}
Quicksort(A,1,n)
[6, 1, 4, 3, 5, 7, 2, 8] # pivot = 6
->
[1, 4, 3, 5, 2, 6, 7, 8]
                .
Quicksort(A,1,5)
Quicksort(A,7,8) ->
[1, 4, 3, 2, 5, 6, 7, 8] # pivot = 1
 .              .  .

Quicksort(A,2,5) ->
[1, 3, 2, 4, 5, 6, 7, 8] # pivot = 4
 .        .     .  .

Quicksort(A,2,3) ->
[1, 2, 3, 4, 5, 6, 7, 8] # pivot = 3
 .  .  .  .  .  .  .  .
\end{verbatim}
   
\section{Lecture VI - Quicksort}

Rozważmy algorytmy służące do dzielenia tablicy w Quicksorcie

\subsection{Lomuto Partition}

\begin{verbatim}
Lomato Partition(A, p, q) # A[p..q]
    pivot = A[p]
    i = p
    for j = p + 1 to q
        if A[j] <= pivot # expensive |A[p..q]| = n, then (n-1) comparisons ~ Theta(n)
            i = i + 1
            swap (A[i], [j]) # expensive, but if dependent
    swap (A[i], A[p]) # pivot in between A[p..i] and A[i+1..q]
    return i

A
|*| <= pivot |i| pivot < |j| ? |
p                              q

We either put the ? element in the '<= pivot' part, or '> pivot' part

A
| <= pivot | * | pivot < |
p                      q
\end{verbatim}

\begin{verbatim}
Example
6, 10, 13, 5, 8, 3, 2, 11
*  i       j

swap(5,10)

6, 5, 13, 10, 8, 3, 2, 11
*  i          j

do nothing

6, 5, 13, 10, 8, 3, 2, 11
*     i          j

swap(3, 13)

6, 5, 3, 10, 8, 13, 2, 11
*     i          j

6, 5, 3, 2, 8, 13, 10, 11
*        i         j   

6, 5, 3, 2, 8, 13, 10, 11
*        i              j

swap(6, 2)

2, 5, 3, 6, 8, 13, 10, 11
*        i              j

return i = 3

\end{verbatim}

Biorąc pod uwagę, że dokonujemy $n-1$ porównań, złożoność Lomuto Partition wynosi $\Theta(n)$.

\subsection{Hoare Partition}

\begin{verbatim}
Hoare Partition(A, p, q) # A[p..q]
    pivot = A[floor((p+q)/2)]
    i = p - 1
    j = q + 1
    while True
        do    
            i++
        while A[i] < pivot

        do 
            j--
        while A[j] > pivot

        if i >= j return j
        swap(A[i], A[j])
\end{verbatim}

\begin{verbatim}
* - pivot

Example
  6, 10, 13, 5, 8, 3, 2, 11
i p          *           q  j
  i          *        j         # swap(6, 2)
  
  2, 10, 13, 5, 8, 3, 6, 11
     i       *     j            # swap(10, 3)

  2, 3, 13, 5, 8, 10, 6, 11
            *

  2, 3, 13, 5, 8, 10, 6, 11
        i   j                   # swap (13, 5)

  2, 3, 5, 13, 8, 10, 6, 11
        *
        j  i
    
A
| <= pivot | < pivot |
p          j         q

return j
\end{verbatim}

W Hoare Partition tracimy pivot który może ulec przesunięciu. Porównań robimy więcej o stałą $n \pm c, c=1$. 
Złożoność $\Theta(n)$ - zdecydowanie mniej swapów, 2-3 razy mniej niż Lomuto partition.

\begin{verbatim}
QS(A,p,q)
    if p < q
        r = Partition(A,p,q)
        QS(A,p,r-1)
        QS(A,r+1,q)
\end{verbatim}

\subsection{Worst Case Analysis for QS}

Najgorzej będzie jak każdorazowo będziemy nierówno dzielić po 1-szym elemencie (odwrotnie posortowana tablica).

\begin{verbatim}
       cn
   /       \
  Theta(1)  c(n-1)
           /    \ 
      Theta(1)  c(n-2)
                   ...
                  /   \
           Theta(1)   Theta(1) 
\end{verbatim}

\begin{align}
    T(n) &= T(n-1) + T(0) + \Theta(n)\\
    T(n) &= T(n-1) + \Theta(n) \leq \sum_{i=0}^{n} c(n-i) + \Theta(1) = \\
    &= c\sum_{i=0}^{n} (n-i) + \Theta(n) =\\
    &= c \frac{(n)(n+1)}{2} + \Theta(n) =\\
    &= O(n^2)
\end{align}

\subsection{Best case Analysis for QS}

Najlepiej będzie jak dzielimy na pół.

\begin{align}
    T(n) &= T(\frac{n}{2}) + T(\frac{n}{2}) + \Theta(n) \\
    T(n) &= 2T(\frac{n}{2}) + \Theta(n) \\
    T(n) &= \Theta(n\log n)
\end{align}

\subsection{Specific case analysis for QS}

\begin{align}
    T(n) &= T(\frac{n}{10}) + T(\frac{9n}{10}) + \Theta(n)
\end{align}

\begin{verbatim}
    cn       | cn
   /  \      | cn
  /    \     | ..
 /      \
         \
          \   
\end{verbatim}

Po zsumowaniu każde piętro będzie miało koszt $cn$. Zchodzimy końca wysokości drzewa.

\begin{align}
    \left(\frac{9}{10}\right)^h n &= 1\\
    n &= \left(\frac{10}{9}\right)^h\\
    h &= \log_{\frac{10}{9}} n
\end{align}

\subsection{Best/Worst case analysis for QS - Intuition}

\begin{align}
    L(n) &= 2 U\left(\frac{n}{2}\right) + \Theta(n)\\
    U(n) &= L(n-1) + L(0) + \Theta(n)\\
\end{align}

Zatem rozwiążmy układ równań:

\begin{align}
    L(n) &= 2(L(\frac{n}{2} - 1) + \Theta(n)) + \Theta (n)
    L(n) &= 2L(\frac{n}{2} - 1) + \Theta(n)
    L(n) = \Theta(n \log n)
\end{align}

\subsection{Average case analysis for QS}

\textit{Rozkład $T_n$ nie jest znany do dziś.}

\begin{align}
    T_n &= \text{\# porównań elementów sortowanej tablicy}, |A|=n\\
    X_k(n) &= \begin{cases}
        1 \quad \text{jeśli partition podzieli tablicę n-elementową na (k, n-k-1)}\\
        0 \quad \text{w p.p.}
    \end{cases}\\
    0 \leq k \leq n - 1
\end{align}

\begin{align}
    E(X_k) = 1\cdot P(X_k=1) + 0\cdot P(X_k=0) = 1\cdot P(X_k=1) = \frac{(n-1)!}{n!} = \frac{1}{n}
\end{align}

Zapiszmy wobec tego równanie na $T_n$

\begin{align}
    T_n &=^{distr.} \begin{cases}
        T_0 + T_{n-1} + n-1 \quad \text{if (0,n-1) - split}\\
        T_1 + T_{n-2} + n-1 \quad \text{if (1,n-2) - split}\\
        \vdots\\
        T_k + T_{n-1-k} + n - 1 \quad \text{if (k,n-k-1) - split}\\
        T_{n-1} + T_{0} + n - 1 \quad \text{if (n-1,0) - split}
    \end{cases}\\
    T_n &=^{distr.} \sum_{k=0}^{n-1} X_k(T_k+T_{n-k-1} + n-1)
\end{align}

\begin{align}
    E(T_n) &= E\left(\sum_{k=0}^{n-1} X_k(T_k+T_{n-k-1} + n-1)\right)=\\
    E(T_n) &= \sum_{k=0}^{n-1} E\left(X_k(T_k+T_{n-k-1} + n-1)\right)=\\
    E(T_n) &= \sum_{k=0}^{n-1} E\left(X_k\right) \cdot E(T_k+T_{n-k-1} + n-1)=\\
    E(T_n) &= \frac{1}{n} \sum_{k=0}^{n-1} E(T_k) + E(T_{n-k-1}) + n-1=\\
    E(T_n) &= \frac{1}{n} \left(\sum_{k=0}^{n-1} E(T_k) + \sum_{k=0}^{n-1} E(T_{n-k-1}) + \sum_{k=0}^{n-1} n-1\right)=\\
    E(T_n) &= \frac{1}{n} \sum_{k=0}^{n-1} E(T_k) + \frac{1}{n} \sum_{k=0}^{n-1} E(T_{n-k-1}) + \frac{1}{n} \sum_{k=0}^{n-1} n-1 = \\
    E(T_n) &= \frac{1}{n} \sum_{k=0}^{n-1} E(T_k) + \frac{1}{n} \sum_{k=0}^{n-1} E(T_{n-k-1}) + n - 1\\
    E(T_n) &= \frac{2}{n} \sum_{k=0}^{n-1} E(T_k) + n - 1\\
    t_n &= \frac{2}{n} \sum_{k=0}^{n-1} t_k + n - 1 \quad \text{rekurencja z pełną historią}\\
\end{align}

Możemy usunąć historię pisząc:

\begin{align}
    n t_n &= 2 \sum_{k=0}^{n-1} t_k + (n-1)n\\
    (n-1) t_{n-1} &= 2 \sum_{k=0}^{n-2} t_k + (n-2)(n-1)\\
    n t_n - (n-1) t_{n-1} &= 2 \sum_{k=0}^{n-1} t_k + (n-1)n - 2 \sum_{k=0}^{n-2} t_k - (n-2)(n-1)\\
    n t_n - (n-1) t_{n-1} &= 2 t_{n-1} + 2 (n-1)\\
    n t_n &= (n+1) t_{n-1} + 2(n-1)\\
    \frac{t_n}{n+1} &= \frac{t_{n-1}}{n} + 2\frac{n-1}{n(n-1)}\\
    f_n &= f_{n-1} + 2\frac{n-1}{n(n+1)}, f_0, f_1 = 0\\
    f_n &= 2 \sum_{k=1}^{n} \frac{k-1}{k(k+1)} =\\
    f_n &= 2 \sum_{k=1}^{n} \frac{2}{k+1} - \frac{1}{k}=\\
    f_n &= 4 \sum_{k=1}^{n} \frac{1}{k+1} + 2\sum_{k=1}^{n} \frac{1}{k}=\\
    f_n &= 4 (H_{n+1} - 1) + 2 H_n\\
    f_n &= 4H_{n+1} + 2H_n - 1\\
    f_n &= 2H_{n} - 4 + \frac{4}{n+1}\\
    E(T_n) = t_n &= (n+1)f_n = 2nH_n + 2H_n - 4(n+1) + 4
\end{align}

\begin{align}
    H_n = \ln n + \gamma + \frac{1}{2n} + \Theta\left(\frac{1}{n^2}\right)
\end{align}

Widzimy, że wiodący czynnik $T_n = 2n\ln n + \Theta(n)$.
Wiemy dlaczego QS jest dobry - średnio wykona $2n\ln n$ porównań asymptotycznie.

\end{document}
